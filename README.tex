\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{listings}
\usepackage{graphicx}
\graphicspath{ {./images/} } 
\author{Nafisa Ali Amir, Abhinav Singh}
\title{Natural Language Programming  \\
	Assignment 1}
\begin{document}
\maketitle
\section*{Q1}
Sentences generated using grammar.gr:
\begin{enumerate}
\item {
	the sandwich pickled a pickle . 
	}

\item {
	a floor with the sandwich with the delicious chief of staff with a sandwich under the sandwich pickled the sandwich . 
	}

\item {
	a pickle kissed the floor ! 
	}

\item {
	the president with a chief of staff under the fine floor under a president with every sandwich under every president in the sandwich under the pickle with the pickle under every floor under every president with the chief of staff with a floor on a floor kissed every sandwich with every president ! 
	}

\item {
	every perplexed chief of staff kissed every floor . 
	}

\item {
	a floor in every pickle on the pickle wanted a president on every sandwich in every pickled floor with every chief of staff under the floor under every chief of staff on every sandwich in a chief of staff with a pickled pickle in every floor in every pickle under a sandwich in every delicious pickle with every chief of staff under a delicious president in the chief of staff in the president on the chief of staff with the floor in the sandwich under a pickle on the floor with every chief of staff with a pickle under every pickle with the floor in a floor under a perplexed floor in the floor under a sandwich under the sandwich under a chief of staff with a president with a chief of staff in a sandwich under every floor with the chief of staff on a president ! 
	}

\item {
	a floor in a delicious pickle on every chief of staff on the chief of staff in every president in a pickle pickled a floor ! 
	}

\item {
	is it true that every floor understood a pickle ? 
	}

\item {
	every chief of staff pickled the chief of staff . 
	}

\item {
	is it true that the pickled delicious pickle wanted the pickle under the sandwich ?
	}
\end{enumerate}
\pagebreak
\section*{Q2a}
Program generates long sentences because of the rule “1\hspace{1cm} NP\hspace{1cm} NP PP”. The reason being recursive nature of the rule itself wherein NP can be expanded into NP itself, leading to longer sentences.
\begin{figure}[h]
\includegraphics[width=\textwidth]{2a1}
\centering
\end{figure}
\pagebreak
\section*{Q2b}
The rule that adds adjectives is “1\hspace{1cm} Noun\hspace{1cm} Adj Noun”.  This rule is essentially expanding Noun. The grammar rules used here have 5 other rules to pick from, to expand a Noun. Thus, the probability of picking up the rule which contains adjectives is reduced to 1/6. To get something like “fine perplexed pickle” where two adjectives appear consequently requires the above rule to be picked up in succession, i.e., one immediately after the other and probability of that further reduces. Hence, program’s sentences do this rarely.
\pagebreak
\section*{Q2c}
The rules have varied counts with floats and ints allowed. An example is given here as "0.4\hspace{1cm}	NP\hspace{1cm}	NP PP". Some sample output sentences are listed below:
\begin{enumerate}
		\item the  sandwich  in  every  president  kissed  the  president  with  the  perplexed  pickle  .
		\item is  it  true  that  a  pickled  president  kissed  a  floor  ? 
		\item is  it  true  that  the  chief  of  staff  ate  a  president  ?
		\item the  sandwich  wanted  every  president  under  a  perplexed  president  .
		\item is  it  true  that  the  chief  of  staff  pickled  a  fine  president  ? 
\end{enumerate}
\pagebreak
\section*{Q2d}
To fix the problem in 2a, i.e., to make the sentences shorter, the number on leftmost side of the rule “1 \hspace{1cm} NP\hspace{1cm} NP PP” should be made less than the number on the leftmost side of the rule “1\hspace{1cm} NP\hspace{1cm} Det Noun”. Decreasing the number in the former rule or increasing the number in the latter rule reduces the probability of the former rule, thus reducing the chances of getting longer sentences. 

To fix the problem in 2b, i.e., to make the sentences have multiple adjectives for the same noun, the number on the leftmost side of the rule “1\hspace{1cm} Noun\hspace{1cm} Adj Noun” should be increased.
\pagebreak
\section*{Q2e}
Modifications to generate more natural sets of sentences were:
\begin{enumerate}
\item {
	Sentences with full stops are more common than question marks
}
\item {
	Exclamatory sentences are least common
}
\item {
	Determiner ‘a’ is common
}
\item {
	Too long sentences are less common
}
\item {
	Adjectives before noun are common
}

\end{enumerate}
Sentences generated after these modifications:
\begin{enumerate}
\item {
	a delicious chief of staff wanted a pickled sandwich . 
}
\item {
	a delicious sandwich on a fine president on the president understood a fine floor ! 
}
\item {
	a sandwich understood a chief of staff under a chief of staff . 
}
\item {
	is it true that a sandwich with every pickled chief of staff pickled every sandwich ? 
}
\item {
	the chief of staff ate a perplexed floor under a president . 
}
\item {
	is it true that every sandwich wanted a president on a pickled sandwich ? 
}
\item {
	a pickle in the fine pickle with every president ate the chief of staff on the floor on a chief of staff . 
}
\item {
	the delicious fine sandwich in a sandwich with the pickled pickle with the president pickled the pickle on the floor . 
}
\item {
	a chief of staff kissed every chief of staff . 
}
\item {
	the chief of staff in the chief of staff understood every floor in every sandwich on a president .
}
\end{enumerate}
\pagebreak
\section*{Q2f}
Probability Problem:\\
The probability of terminating is 1/2 for the following grammar:\\
1\hspace{1cm}	S\hspace{1cm}	x\\
2\hspace{1cm}	S\hspace{1cm}	S S\\

Each link in the table below represents a choice of expansion for S\\
\begin{figure}[h]
	\includegraphics[width=\textwidth]{2f1}
	\centering
\end{figure}
Let P(s) = Probability of terminating\\
\\
From the tree above,\\ 
\begin{equation}
\label{eq:2f_e1}
		P(s) = \frac{1}{3} + \frac{2}{3}*\{\frac{1}{3} + \frac{2}{3}*\{...\}*\frac{2}{3}*\{...\}\}*\{\frac{1}{3} + \frac{2}{3}*\{...\}*\frac{2}{3}*\{...\}\}
\end{equation}
\\
The above equation is essentially an infinite series which can be written in short as follows:
\\
\begin{equation}
\label{eq:2f1}
P(s) = \frac{1}{3} + \frac{2}{3}P(s)^2
\end{equation}

\begin{equation}
\label{eq:2f2}
1 - 3P(s) + 2P(s)^2 = 0
\end{equation}
Solving the equation \ref{eq:2f2} for P(s), we get \(P(s) = \frac{1}{2}\)

\pagebreak
\section*{Q3a}
Given: Sally ate a sandwich .\\
Modification: Proper nouns are specific types of nouns which generally do not follow a determiner. However they still can be a part of a noun phrase. Thus adding a rule to expand a noun phrase as a proper noun, noted in the grammar file as PrNoun.\\
Rule: ``2\hspace{1cm}	NP\hspace{1cm} PrNoun"\\
Rule: ``1\hspace{1cm}	PrNoun\hspace{0.4cm}Sally"\\
Output: Sally pickled pickle and a chief of staff .\\
\pagebreak
\section*{Q3b}
Given: Sally and the president wanted and ate a sandwich .\\
Modification 1: Noun phrase can be made up of two noun phrase with a conjunction in between\\
Modification 2: Verb phrase can include two verbs joined by a conjunction
Rule 1: ``0.3\hspace{1cm}	NP\hspace{1cm}	NP CC NP"\\
Rule 2: ``0.4\hspace{1cm}	Verb\hspace{1cm} Verb CC Verb "\\
Output 1: it wanted perplexed pickle and a very delicious sandwich .\\
Output 2: it wanted and understood Sally .\\
\pagebreak
\section*{Q3c}
Given: the president sighed .\\
Modification: A sentence can be formed of a noun phrase followed by an intransitive verb. The intransitive verb does not have a direct object.\\
Rule: ``0.4\hspace{1cm}	S\hspace{1cm}	NP V\_it"\\
Output: delicious floor or the very pickled chief of staff sighed .\\
\pagebreak
\section*{Q3d}
Given: the president thought that a sandwich sighed .\\
Modification: A verb phrase can be formed of verb with relative clause describing the verb joined by ‘that’. The relative clause can be a sentence in itself.\\
Rule 1: ``0.5\hspace{1cm}	VP\hspace{1cm}	Verb Rel S "\\
Rule 2: ``1\hspace{1cm}		Rel\hspace{1cm}	that"\\
Output: every delicious chief of staff understood that president understood the president .\\
\pagebreak
\section*{Q3e}
Given: it perplexed the president that a sandwich ate Sally .\\
Modification: A sentence is formed by a noun phrase followed by verb phrase. Likewise a sentence can also be formed by a pronoun (like he, she, it) followed by verb phrase.\\
Rule 1: ``0.5\hspace{1cm}	S \hspace{1cm}	PRN VP "\\
Rule 2: ``0.1\hspace{1cm}	VP\hspace{1cm}	VP Rel S"\\
Output: it  kissed  that  president  understood  pickled  president  in  the  floor  or  chief  of  staff  ! \\
\pagebreak
\section*{Q3f}
Given: that a sandwich ate Sally perplexed the president .\\
Modification: A sentence can start with ‘that’ followed by a relative clause which describes a verb phrase following it\\
Rule: `0.2\hspace{1cm}	S\hspace{1cm} Rel S VP"\\
Output: that the president and a pickle sighed kissed and ate Sally . \\
\pagebreak
\section*{Q3g}
Given: the very very very perplexed president ate a sandwich .\\
Modification: An adjective can be formed by an adverb followed by an adjective\\
Rule: `0.6\hspace{1cm}	Adj\hspace{1cm}	Adv Adj"\\
Output: every very delicious chief of staff thought that perplexed fine floor and a chief of staff in a pickle and Sally kissed floor .\\
\pagebreak
\section*{Q3h}
Given: the president worked on every proposal on the desk .\\
Modification 1: A verb phrase can have an intransitive verb with a prepositional phrase describing it\\
Modification 2: A verb phrase can have another verb phrase with a prepositional phrase describing it\\
Rule 1: ``0.4\hspace{1cm}	VP\hspace{1cm}	V\_it PP"\\
Rule 2: ``0.4\hspace{1cm}	VP\hspace{1cm}	VP PP"\\
Output: Sally  or  chief  of  staff  sighed  on  every  pickle  in  every  delicious  sandwich  .\\
\\
Few more sentences generated from the modifications are:\\
\\
Output: sandwich  on  the  chief  of  staff  in  very  perplexed  pickled  sandwich  under  Sally  and  a  floor  or  Sally  wanted  and  kissed  every  floor  . 
\\
The above sentence follows rule from Q3a, Q3g, Q3b.\\
\\
Output: a  perplexed  floor  under  a  sandwich  sighed  .
\\
The above sentence follows rule from Q3c.\\
\\
Output: that  it  worked  in  a  chief  of  staff  under  pickle  kissed  floor  . 
\\
The above sentence follows rule from Q3e, Q3f, Q3h.\\
\\
Output: a  perplexed  pickle  ate  that  every  floor  worked  with  a  sandwich  ! 
\\
The above sentence follows rules from Q3d, Q3h.\\
\pagebreak
\section*{Q4}
Following 5 sentences have been generated in tree format:\\
\lstinputlisting[
        gobble=4,
        breaklines,
        breakautoindent=false,
        breakatwhitespace,
        breakautoindent,
        %numbers=left,
        basicstyle=\ttfamily\mdseries
        ]{q4.txt} % modify this to include your own generated trees
\pagebreak
\section*{Q5a}
The other derivation is as follows:
\lstinputlisting[
        gobble=4,
        breaklines,
        breakautoindent=false,
        breakatwhitespace,
        breakautoindent,
        %numbers=left,
        basicstyle=\ttfamily\mdseries
        ]{q5a.txt} % modify this to include your own generated trees
\pagebreak
\section*{Q5b}
The reason to care which derivation was used is the meaning of the sentence. In the derivation shown in the question, it is clear that the noun which is 'on the floor’ is sandwich. However, in the other derivation, ‘pickle’ is the noun which is ‘on the floor’. The prepositional phrase can be talking about either of the noun phrases, ‘every sandwich with a pickle’ or ‘with a pickle on the floor’ depending on the derivation. Deriving differently changes the meaning of the sentence.\\
\pagebreak
\section*{Q6a}
The parser does not always recover the derivation as intended by randsent. This is illustrated by taking the following example.\\
“every pickle on a president in the sandwich under a president wanted the pickle .” This sentence was generated by randsent with the following derivation:\\
\lstinputlisting[
        gobble=4,
        breaklines,
        breakautoindent=false,
        breakatwhitespace,
        breakautoindent,
        %numbers=left,
        basicstyle=\ttfamily\mdseries
        ]{q6a1.txt} % modify this to include your own generated trees			
However, the parser picked a different derivation shown below.\\
\lstinputlisting[
        gobble=4,
        breaklines,
        breakautoindent=false,
        breakatwhitespace,
        breakautoindent,
        %numbers=left,
        basicstyle=\ttfamily\mdseries
        ]{q6a2.txt} % modify this to include your own generated trees	
Randsent generated sentence with the derivation which was not the highest probability derivation while the parser picked up the highest probability derviation that led to misunderstanding or ambiguity.\\
\pagebreak
\section*{6b}
``Every sandwich with a pickle on the floor under the chief of staff” is a noun phrase and can also be written as “NP Prep NP Prep NP Prep NP”. The two rules under the original grammar that can be used to derive the above format is “NP - NP PP” and “PP - Prep NP”. There can be five ways to derive the above sentence using these two rules and they are shown below:\\
\begin{figure}[h]
	\label{fig:6b1}
	\includegraphics[width=\textwidth]{6b1}
\end{figure}
\begin{figure}[h]
	\label{fig:6b2}
	\includegraphics[width=\textwidth]{6b2}
\end{figure}
\begin{figure}[h]
	\label{fig:6b3}
	\includegraphics[width=\textwidth]{6b3}
\end{figure}
\begin{figure}[h]
	\label{fig:6b4}
	\includegraphics[width=\textwidth]{6b4}
\end{figure}
\begin{figure}[h]
	\label{fig:6b5}
	\includegraphics[width=\textwidth]{6b5}
\end{figure}
\pagebreak
\\
\\
\pagebreak
\\
\\
\pagebreak
\section*{6c}
The sentences generated using grammar.gr were:\\
\begin{enumerate}
\item is  it true  that a sandwich  under a president  under every pickled  floor under the pickle  under a chief of staff with  a fine perplexed fine perplexed  floor wanted a floor under every  sandwich ?\\
42 Parsers\\
\item every  sandwich  under a delicious  fine pickle on a perplexed  pickle understood every fine delicious  chief of staff in the president !\\
2 Parsers\\
\item is  it true  that the pickle  pickled a pickle in  a perplexed fine sandwich  in the pickle on a floor under  every pickle with every chief of staff  on a chief of staff in the chief of staff  with every pickle under the president under a  president on every chief of staff ?\\
58786 Parsers\\
\end{enumerate}
The parses generated were more for those with more prepositions and were using the rule “NP - NP PP” and “PP - Prep NP”.  Another thing to note is that number of parsers also increase when more prepositional phrase are adjacent to one another.\\
\pagebreak
\section*{Q6d.i}
The first sentence is made up of 6 words including punctuation mark, where word1 = the, word2 = president, word3 = ate, word4 = the, word5 = sandwich, word6 = .\\
The sentence can be derived as follows:\\
\begin{figure}[h]
	\includegraphics[width=\textwidth]{6d1}
	\centering
\end{figure}
\\
In the tree, probability for each expansion, according to grammar.gr is written next to the symbol.\\
P(best\_parse) is actually the joint probability of all the words in the sentence, which can be written as follows by chain rule.\\
\\
\(P(best\_parse) = P(word1,word2,word3,word4,word5,word6)\)\\
\\
\(P(best\_parse) = P(word1)*P(word2|word1)*P(word3|word1,word2)...\)\\
\\
\(P(word1) = \frac{1}{3}*\frac{1}{2}*\frac{1}{3}\)\\
\\
\(P(word2|word1) = \frac{1}{6}\)\\
\\
\(P(word3|word1,word2) = \frac{1}{5}\)\\
\\
\(P(word4|word1,word2,word3) = \frac{1}{2}*\frac{1}{3}\)\\
\\
\(P(word5|word1,word2,word3,word4) = \frac{1}{6}\)\\
\\
\(P(word6|word1,word2,word3,word4,word5) = 1\)\\
\\
From the above equations,\\
\(P(best\_parse) = \frac{1}{3}*\frac{1}{2}*\frac{1}{3}*\frac{1}{6}*\frac{1}{5}*\frac{1}{2}*\frac{1}{3}*\frac{1}{6}= 5.144032922e-05\) \\
\\
P(sentence) is equal to P(best parse) because that is the only derivation for the sentence using the grammar ‘grammar.gr’. Hence, given that the sentence was generated, the probability of it being the best parse is 1. It can also be derived as follows:\\
\\
\(P(best\_parse|sentence) = \frac{P(best\_parse, sentence)}{P(sentence)}\)\\
\\
Since \(P(best\_parse) = P(sentence)\),\\
\\
\(P(best\_parse, sentence) = P(best\_parse) = P(sentence)\) \\
\\
\(P(best\_parse|sentence) = 1\)
\section*{Q6d.ii}
The second sentence could have been derived in two ways with equal probabilities, which is why given the sentence was generated, probability that it was the best parse is exactly 0.5. It can also be derived as follows:\\
\\
\(P(sentence) = 1.240362877e-09\)\\
\\
\(P(deriv1) = 6.201814383e-10\)\\
\\
\(P(deriv2) = 6.201814383e-10\)\\
\\
\(P(best\_parse|sentence) = \frac{P(best\_parse, sentence)}{P(sentence)}\)\\
\\
Since \(P(best\_parse, sentence) = P(best\_parse) = 6.201814383e-10\),\\
\\
\(P(best\_parse|sentence) = \frac{P(best\_parse)}{P(sentence)}\)\\
\\
\(P(best\_parse|sentence) = 0.5\)
\\
\section*{Q6d.iii}
p(x) and q(x) are two probability distributions of the corpus, p(x) being the true probability and q(x) being the probability of grammar predicting the corpus. q(x) can be further written as product of probabilities of the two sentences, since the two events are independent.
\\
From Q 6d (i) and (ii),\\
\begin{equation}
\label{eq:6d1}
P(sentence1) = 5.144032922e-05\\
\end{equation}
\begin{equation}
\label{eq:6d2}
P(sentence2) = 1.240362877e-09\\
\end{equation}
\\
From equations \ref{eq:6d1} and \ref{eq:6d2},\\
\begin{equation}
\label{eq:6d3}
q(x = given corpus) = 6.380467474e-14\\
\end{equation}
\\
Since the corpus is already given and it is the only corpus given, the probability p(x) is given as,\\
\begin{equation}
\label{eq:6d4}
p(x = given corpus) = 1
\end{equation}
\\
\\
Cross Entropy is given as,\\
\begin{equation}
\label{eq:6d5}
	Cross Entropy = -\sum_{i=1}^{N}p(x_i)\log_2 q(x_i)\\
\end{equation}
where i is the \(i_{th}\) corpus and N is the number of corpuses which is 1 here.\\
\begin{equation}
\label{eq:6d6}
	Cross Entropy = -p(x=given corpus)\log_2 q(x=given corpus)
\end{equation}
\\
From equations \ref{eq:6d3}, \ref{eq:6d4} and \ref{eq:6d6},\\
Cross Entropy = 43.83333 bits per corpus\\
\\
To find the cross entropy per word, we can divide the cross entropy per corpus by number of words in the corpus which is 18.\\
\\
Cross Entropy per word = 43.83333/18 = 2.435185067 bits per word\\
\section*{Q6d.iv}
Perplexity per word is defined as 2 to the power of entropy. Using the cross entropy derived above in Q6d iii, 
\\
Perplexity per word will be \(2^{2.435185067} = 5.408337\)\\
\section*{Q6d.v}
Probability of sentence2 is 0 which makes cross entropy infinity. In other words, compression program will need infinite number of bits for compression. Without as many bits, it may not be able to compress the sentences very well.\\
\pagebreak
\section*{Q6e}
Entropy of grammar2 is 2.173970616 bits = -(-400.0105934 log-prob. / 184 words). The command used were\\
\\
./randsent (grammar2 file path) 10 \(>\) (file containing sentences generated by grammar2)\\
cat (file containing sentences generated by grammar2) \(|\) ./parse -P -g (grammar2 file path) \(|\) tail -n -1\\
\\
The last line of the output file contains the entropy value.\\
Entropy of grammar3 is 2.440343114 bits = -(-224.5115665 log-prob. / 92 words)\\
Entropy of grammar3 is higher than grammar2 since grammar3 allows more variety of sentences to be generated.Consequently, grammar3 needs more number of bits to encapsulate the information about the sentences than grammar2.\\
Entropy of grammar is 2.019856004 bits = -(-1070.523682 log-prob. / 530 words). However, entropy of grammar turns up most of the time to be infinite bits. The reason being the long sentences that it generates are terminated midway by the randsent program with dots. Since there are no rules in the grammar file to handle the dots and incomplete sentences, it predicts the probability of those sentences to be zero which results in the infinite entropy.\\
\pagebreak
\section*{Q6f}
The sentences generated in Q6e by grammar2 are parsed with grammar3 and grammar to compute the cross entropies.\\
Cross entropy of grammar3 is 2.834063887 bits = -(-521.4677553 log-prob. / 184 words)\\
Cross entropy of grammar is 2.226009522 bits = -(-409.5857521 log-prob. / 184 words)\\
\\
Since there will be less uncertainity in probability distribution of the grammar predicting the sentences which generated them than the probability distribution of another grammar predicting them, the entropy is less than cross entropies. In other words, the generated sentences are more likely to be generated by grammar2 than other grammar files, thus less number of bits can suffice for grammar2 than grammar3 or grammar, resulting in lower entropy relative to cross entropies. It can also be shown by the fact that cross entropy is equal to the sum of entropy and the KL divergence of the two probability distributions.\\
Another thing to note here is that cross entropy of grammar3 is higher than grammar. The reason being additional rules in grammar3 which can create more variety of sentences. Thus to generate those specific sets of sentences, grammar3 needs more number of bits than grammar.
\pagebreak
\section*{Q7a}
This implementation required us to split POS tags into subcategories. This led to creating further more copies of other affected rules to cope with different combinations of possible legal POS tags. Specifically, there are two copies of determiners and nouns created, which further leads to the creation of two sets of adjectives and adverbs as well. These affected symbols cause the grammar rules to be duplicated and catered for each of them specifically. An example of rule is "1\hspace{1cm}	NPS\hspace{1cm}	Det\_a Noun\_a" and "1\hspace{1cm}	NPS\hspace{1cm}	Det\_an Noun\_an"Some outputs are listed below:\\
\begin{enumerate}
	\item \{ they  select [ the  presidents  under [ Sally ]]\} .  
	\item \{ Sally  eats [ an  apple ] under [ the  citizens ]\} . 
	\item \{ an  apple  wants [[ citizens ] and [ the  presidents ]] with [ an  apple ]\} ! 
	\item is  it  true  that \{ they  choose [ the  apples ]\} ? 
	\item \{ they  run [ a  sandwich ]\} ! 
\end{enumerate}
\pagebreak
\section*{Q7b}
Here, we have implemented verb tenses for simple present, simple past, simple future, perfect present, perfect past, perfect future, progressive present, progressive past, progressive future, perfect progressive present, perfect progressive future. This one is similar to the one mentioned in 7f. Some output sentences generated are:\\
\begin{enumerate}
	\item is  it  true  that  a  single  chief  of  staff  will  have  been  eating  Sally  ?
	\item a  single  chief  of  staff  will  be  eating  single  chief  of  staff  . 
	\item is  it  true  that  it  was  eating  a  single  president  ? 
	\item is  it  true  that  single  chief  of  staff  had  eaten  the  apples  ? 
	\item it  will  be  eating  Sally  under  single  president  under  the  citizens  on  a  single  delicious  apple  .
\end{enumerate}
\pagebreak
\section*{Q8a}
In this section, we took the liberty of adding optional terminals in the grammar contained within paranthesis. An example rule in the CFG looks like "1\hspace{1cm} NPS\hspace{1cm}(Det\_a) (Quant\_s) Noun\_a", where the two leftmost columns still mean the same. However the last column now includes some preterminals within paranthesis which are optional. A new set of preterminals are added as well called as quantifiers which includes many, few, single, etc. These are also classified as singlular and plural for agreement with the corresponding verbs. The sentence generator is modified to count for the rules containing paranthesis and to expand them for all possible combinations of expansions. Some output sentences generated after the modifications (with the -b option on) are:\\
\begin{enumerate}
	\item \{ they  select [ a  single  president ]\} . 
	\item \{ it  chooses [ some  presidents ]\} .
	\item is  it  true  that \{ they  select [ a  single  delicious  apple ] with [ several  presidents  under [ the  presidents ]]\} ?
	\item \{[ an  ambivalent  ambivalent  president  in [ several  citizens ]] sighed \} .
	\item \{ that { they  select [ a  few  citizens ]} chooses  a  single  sandwich \} .
\end{enumerate}
\section*{Q8b}
In this section, the grammar is updated to allow for some sentences that can be answered in yes and no. This rule is similar to the one mentioned in 7b. Some sentences generated from the above modifications in the grammar are:\\
\begin{enumerate}
	\item did  many  citizens  take  a  lot  of  presidents  ?  
	\item will  some  apples  eat  the  citizens  ?  
	\item will  the  citizens  drink  several  presidents  ?  
	\item did  a  single  delicious  ambivalent  apple  eat  the  citizens  ?  
	\item did  many  apples  drink  single  sandwich  ?  
\end{enumerate}
\section*{Q8c}
This section lists the output of the implementation of rules that allow for singular plural agreement. For implementation purposes, copies of the rules are created, similar to that in the case of determiner, one for singular nouns and verbs and other for plural ones. A set of example rule is "1\hspace{1cm}	VPS\hspace{1cm}	Verb\_s NP" and "1\hspace{1cm}	VPP\hspace{1cm}	Verb\_p NP". Some output are as follows:\\
\begin{enumerate}
	\item is  it  true  that \{[ the  apples  on [ the  presidents ]] worked \} ?  
	\item is  it  true  that \{ the  presidents  select [ the  presidents ]\} ? 
	\item \{ they  select [ every  delicious  chief  of  staff ]\} . 
	\item \{ the  presidents  choose [ Sally ]\} ! 
	\item \{ every  ambivalent  president  eats [[ the  citizens ] and [ the  citizens ]]\} . 
\end{enumerate}
\section*{Q8d}
This section shows the implementation of Relative clauses using the word 'that'. This one is similar to the one mentioned in 7c. Some sentences generated are:\\
\begin{enumerate}
	\item the  apples  that  a  single  sandwich  chooses  has  been  eating  the  apples  !  
	\item it  was  eating  a  delicious  ambivalent  apple  that  had  been  eating  single  president  under  a  lot  of  apples  on  a  sandwich  with  a  single  chief  of  staff  . 
	\item it  eats  a  single  sandwich  that  Sally  chooses  apples  . 
	\item they  choose a  single  very  ambivalent  very  ambivalent  ambivalent  apple that a  single  sandwich  has  eaten the  presidents  .
\end{enumerate}


\end{document}
